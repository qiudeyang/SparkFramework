## 文件解析

spark支持的一些常见格式

|样式名称|结构化|备注|
|---|---|---|
|文本文件|否|普通的文本文件，每行一条记录|
|JSON|半结构化|常见的基于文本的格式，半结构化；大多数库都要求每行一条记录|
|CSV|是|非常常见的基于文本的格式，通常在电子表格应用中使用|
|SequenceFiles|是|一种用于键值对数据的常见Hadoop文件格式|
|Protocol buffers|是|一种快速、节约空间的跨语言格式|
|对象文件|是|用来将Spark作业中的数据存储下来以让共享的代码读取。改变类的时候它会失效，因为它依赖于Java虚拟化|

### 文本文件  

读取文本文件

在Spark中读写文本文件很容易，当我们把一个文本文件读取为RDD，输入的每一行都会成为RDD的一个元素。也可以把多个完整的文本文件一次性读取为一个pair RDD，且支持通配符，其中键是文件名，值是文件内容。如果文件足够小，那么可以使用SparkContext.wholeTextFiles（）方法，该方法会返回一个pair  RDD，其中键是输入文件的文件名。

保存文本文件

saveAsTextFile（）方法接收一个路径，并将RDD中的内容都输入到路径对应的文件中。Spark会将传入的路径作为目录对待，会在那个目录下输出多个文件。这样，Spark就可以从多个节点上并行输出了。在这个方法中，我们不能控制数据的哪一部分输出到哪个文件中，不过有些输出格式支持控制。

JSON

读取JSON数据的最简单的方式是将数据作为文本文件读取，然后使用JSON解析器来对RDD中的值进行映射操作。

保存JSON：
可以使用之前将字符串RDD转为解析好的JSON数据的库，将由结构化数据组成的RDD转为字符串RDD，然后使用Spark的文本文件API写出去。
result.filter(p => p.lovesPandas).map(mapper.writeValuesAsString()).saveAsTextFile(outputFile)

### 逗号分隔值与制表符分隔值

逗号分隔值（CSV）文件每行都由固定数目的字段，字段间用逗号隔开（在制表符分隔值文件，即TSV文件中用制表符隔开）。记录通常是一行一条，不过也不总是这样，有时也可以跨行。

与JSON中的字段不一样的是，这里的每条记录都没有相关联的字段名，只能得到对应的序号，常规做法是使用第一行中每列的值作为字段名。

读取CSV：
读取CSV/TSV数据和读取JSON数据类似，都需要先把文件当作普通文本文件来读取数据，再对数据进行处理。

如果你的CSV的所有数据字段均没有包含换行符，你也可以使用textFile（）读取并解析数据。

import   java.io.StringReader

import   au.com.byte.opencsv.CSVReader

...

val  input = sc.textFile(inputFile)

val  result  = input.map{ line =>

  val  reader = new CSVReader(new   StringReader(line));

  reader.readNext();

}
