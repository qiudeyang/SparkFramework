# RDD简介

RDD其实就是分布式的元素集合。

在Spark中，对数据的所有操作不外乎创建RDD，转化已有RDD以及调用RDD操作进行求值。而在这一切背后，Spark会自动将RDD中的数据分发到集群上，并将操作并行化执行。

Spark中的RDD就是一个不可变的分布式对象集合。每个RDD都被分为多个分区，这些分区运行在集群中的不同节点上。

## 创建RDD的两种方法

读取一个外部数据集或在驱动器程序里分发驱动器程序中的对象集合（比如list和set）

## RDD操作

RDD支持两种类型的操作：转化操作（transformation）和行动操作（action）。转化操作会由一个RDD生成一个新的RDD。行动操作会对RDD计算出一个结果或将结果返回到驱动器程序中，或把结果存储到外部存储系统（如HDFS）中。

区分方法：区分一个特定的函数是转化操作还是行动操作，只需要看它的返回值类型，转化操作返回的是RDD，而行动操作返回的是其他的数据类型。

转化操作可以操作任意数量的输入RDD，且不改变原有的RDD。

虽然你可以在任何时候定义新的RDD，但Spark只会惰性地计算这些RDD，它们只会在第一次在一个行动操作中用到时，才会真正计算。事实上，在行动操作first（）中，Spark只需要扫描文件直到找到第一个匹配的行为止，而不需要读取整个文件。

默认情况下，Spark的RDD会在你每次对它们进行行动操作时重新计算。如果想在多个行动操作中重用同一个RDD，可以使用RDD.persist( )rangSpark把这个RDD缓存下来。在第一次对持久化的RDD计算之后，Spark会把RDD的内容保存到内存中（以分区方式存储到集群上的各机器上），这样在之后的行动操作中，就可以重用这些数据了。

总的来说，每个Spark程序或shell会话都按如下方式工作：

（1）从外部数据创建出输入RDD

（2）使用诸如filter()这样的转换操作对RDD进行转化，以定义新的RDD

（3）告诉Spark对需要被重用的中间结果RDD执行persist()操作

（4）使用行动操作（例如count( )和first( )等）来触发一次并行计算，Spark会对计算进行优化后再执行。

RDD中有一个collect( )函数，可以用来获取整个RDD中的数据。只在数据量小时使用。

需要注意的是，每当我们调用一个新的行动操作时，整个RDD都会从头开始计算，要避免这种低效的行为，用户可以将中间结果持久化。

## 创建RDD最简单的方式

把程序中一个已有的集合传递给SparkContext的parallelize( )方法。
val  lines = sc.parallelize(List("pandas","i like pandas"))

## 惰性求值

惰性求值意味着当我们对RDD调用转化操作（例如调用map( )）时，操作并不会马上执行。相反，Spark会在内部记录下所要求执行的操作的相关信息。我们不应该把RDD看做是存放着特定数据的数据集，而最好把每个RDD当作我们通过转化操作构建出来的、记录如何计算数据的指令列表。把数据读取到RDD的操作同样是惰性的，因此，当我们调用sc.textFile( )时，数据并没有读取进来，而是在必要时才会读取。和转化操作一样，读取数据的操作也有可能多次执行。

## 向Spark传递函数

scala：与Python类似，传递一个对象的方法或字段时，会包含对整个对象的引用。我们可以把需要的字段放到一个局部变量中，来避免传递包含该字段的整个对象。
如果在scala中出现了NotSerializableException，通常问题在于我们传递了一个不可序列化的类中的函数或字段，记住，传递局部可序列化变量或顶级对象中的函数始终是安全的。

# 基本RDD

## 针对各个元素的转化操作（受任意数据类型的RDD支持）
`map（）`
接收一个函数，把这个函数用于RDD中的每个元素，将函数的返回结果作为结果RDD中对应元素的值。map（）的返回值类型不需要和输入类型一样。

举例：

val  input = sc.parallelize(list(1,2,3,4))

val  result = input.map(x => x* x)

println(result.collect().mkString(","))

`filter（）`

接收一个函数，并将RDD中满足该函数的元素放入到新的RDD中返回。

`flatMap（）`

实现对每个输入元素生成多个输出元素，与map（）类似，我们提供给flatMap（）函数被分别应用到了输入RDD的每个元素上，不过返回的不是一个元素，而是一个返回值序列的迭代器。得到的是一个包含各个迭代器可访问的所有元素的RDD。简单用途是：把输入的字符串切分为单词。
把flatMap（）看做是将返回的迭代器“拍扁”，这样就得到了一个由各列表中的元素组成的RDD，而不是一个由列表组成的RDD。细化。

## 伪集合操作（要求操作的RDD是相同数据类型的）

`distinct( )`

去重，生成一个只包含不同元素的新RDD。开销大，因为它需要把所有数据通过网络进行混洗（shuffle），以确保每个元素都只有一份。

`union(other)`

返回一个包含两个RDD中所有元素的RDD，可能会有重复数据，如有必要，用distinct（）去重。

`intersection(other)`

返回两个RDD中都有的元素，自带去重功能。性能差，因为它需要通过网络混洗数据来发现共有的元素。

`subtract（other）`

只返回由只存在于第一个RDD中而不存在于第二个RDD中的所有元素组成的RDD

`cartesian（笛卡尔积）`

返回两个RDD随机取出一个元素的组合

## 行动操作

`reduce`
接收一个函数作为参数，这个函数要操作两个RDD的元素类型的数据并返回一个同样类型的新元素，最简单就相加。

`fold（）`

fold()和reduce（）类似，接收一个与reduce（）接收的函数签名相同的函数，再加上一个“初始值”来作为每个分区第一次调用时的结果。你所提供的初始值应当是你提供的操作的单位元素，也就是说，使用你的函数对这个初始值进行多次运算都不会改变结果（例如+对应0，\*对应1，或者拼接操作对应的空列表）

fold（）和reduce（）都要求函数的返回值类型需要和我们所操作的RDD中的元素类型相同。

`aggregate（）`

返回值类型可不同，不过需要提供我们期待返回的类型的初始值

RDD的一些行动操作会以普通集合或者值得形式将RDD的部分或全部数据返回驱动器程序中。

collect（）会将整个RDD的内容返回。collect（）通常在单元测试中使用，因为此时RDD的整个内容不会很大，可以放在内存中。collect（）要求所有数据都必须能一同放入单台机器的内存中。

`take（n）`

返回RDD中的n个元素，并且尝试只访问尽量少的分区。

如果为数据定义了顺序，可以使用top（）从RDD中获取前几个元素，top（）会使用数据的默认顺序，但我们也可以提供自己的比较元素，来提取前几个元素。

`takeSample（withReplacement,num,seed）`函数可以让我们从数据中获取一个采样，并制定是否替换。

`foreach（）`行动操作来对RDD中的每个元素进行操作，而不需要把RDD发回本地。

`count（）`用来返回元素的个数

`countByValue（）`返回一个从各值到值对应的计数的映射表

# 在不同RDD类型间转换

在scala中，将RDD转为有特定函数的RDD（比如在RDD[Double]上进行数值操作）是由隐式转换来自动处理的。需要加上import  org.apache.spark.SparkContext.\_来使用这些隐式转换。这些隐式转换可以隐式地将一个RDD转为各种封装类，比如DoubleRDDFunctions（数值数据的RDD）和PairRDDFunctions（键值对RDD），这样我们就有了诸如mean（）和variance（）之类的额外的函数。

# 持久化（缓存）

当我们让Spark持久化计算一个RDD时，计算出RDD的节点会分别保存它们所求出的分区数据。如果一个有持久化数据的节点发生故障，Spark会在需要用到缓存的数据时重算丢失的数据分区，如果希望节点故障的情况不会拖累我们的执行速度，也可以把数据备份到多个节点上。在Scala和Java中，默认情况下persist( )会把数据以序列化的形式缓存在JVM的堆空间中。

在scala中使用`persist（）`

val  result = input.map(x => x * x)

result.persist( )

println(result.count( ))

println(result.collect().mkString(","))

注意：我们在第一次对这个RDD调用行动操作前就调用了persist（）方法，persist( )调用本身不会触发强制求值。
RDD中还有一个方法unpersist( )，调用该方法可以手动把持久化的RDD从缓存中去掉。
