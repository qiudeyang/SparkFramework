## 创建Pair RDD
需要把一个普通的RDD转为Pair

RDD时，可以调用map（）函数来实现，传递的函数需要返回键值对。

Pair RDD的转化操作

Pair RDD可以使用所有标准RDD上的可用的转化操作。

### Pair RDD的转化操作（以键值对集合{（1,2），（3,4），（3,6）}为例）

|函数名|目的|示例|结果|
|---|---|---|---|
|reduceByKey(func)|合并具有相同键的值|rdd.reduceByKey((x,y) => x+y)|{(1,2),(3,10)}|
|groupByKey( )|对具有相同键的值进行分组|rdd.groupByKey( )|{(1,[2]),(3,[4,6])}|
|mapValues(func)|对Pair RDD中的每个值应用一个函数而不改变键|rdd.mapValues(x => x+1)|{(1,3),(3,5),(3,7)}|
|keys()|返回一个仅包含键的RDD|rdd.keys()|{1,3,3}|
|values()|返回一个仅包含值得RDD|rdd.values()|{2,4,6}|
|sortByKey( )|返回一个根据键排序的RDD|rdd.sortByKey（）|{（1,2）,(3,4),(3,6)}|


### 针对两个Pair RDD的转化操作（rdd={(1,2),(3,4),(3,6)} other ={(3,9)}）

|函数名|目的|示例|结果|
|---|---|---|---|
|subtractByKey|删掉RDD中键与other RDD中的键相同的元素|rdd.subtractByKey(other)|{(1,2)}|
|join|对两个RDD进行内连接|rdd.join(other)|{(3,(4,9)),(3,(6,9))}|
|rightOuterJoin|对两个RDD进行连接操作，确保第二个RDD的键必须存在|rdd.rightOuterJoin(other)|{(3,(Some(4),9)),(3,(Some(6),9))}|
|leftOuterJoin|对两个RDD进行连接操作，确保第二个RDD的键必须存在|rdd.leftOuterJoin(other)|{(1,(2,None)),(3,(4,Some(9))),(3,(6,Some(9)))}|
|cogroup|将两个RDD中拥有相同键的数据分组到一起|rdd.cogroup(other)|{(1,([2],[])),(3,([4,6],[9]))}|

### 聚合操作

reduceByKey( )  foldByKey( )

计算每个键对应的平均值：
val  tmpresult = rdd.mapValues( x => (x,1)).reduceByKey((x,y) =>
(x._1+y._1,x._2+y._2))

val finalresult = tmpresult.mapValues(x => x._1/x._2)

println(finalresult.collect().mkString(" "))

combineByKey（）是最为常用的基于键进行聚合的函数，大多数基于键聚合的函

数都是用它来实现的。和aggregate（）一样，combineByKey()可以让用户返回与输入数据的类型不同的返回值。

由于combineByKey( )会遍历分区中的所有元素，因此每个元素的键要么还没有遇到过，要么就和之前的某个元素的键相同。如果这是一个新的元素，combineByKey（）会使用一个叫做createCombiner（）的函数来创建那个键对应的累加器的初始值。需要注意的是，这一过程会在每个分区中第一次出现各个键时发生，而不是在整个RDD中第一次出现一个键时发生。

如果这是一个在处理当前分区之前已经遇到的键，它会使用mergeValue()方法将该键的累加器对应的当前值与这个新的值进行合并。

由于每个分区都是独立处理的，因此对于同一个键可以有多个累加器。如果有两个或者更多的分区都有对应同一个键的累加器，就需要使用用户提供的mergeCombiner（）方法将各个分区的结果进行合并。

并行度调优：每个RDD都有固定数目的分区，分区数决定了在RDD上执行操作时的并行度。

数据分组：
groupByKey：

对于一个由类型K的键和类型V的值组成的RDD，所得到的结果RDD类型完全会是[K,Iterable[V]]

除了对单个RDD的数据进行分组，还可以使用一个叫作cogroup（）的函数对多个共享同一个键的RDD进行分组。对两个键的类型均为K而值的类型分别为V和W的RDD进行cogroup（）时，得到的结果RDD类型为[(K,(Iterable[V],Iterable[W]))]。如果其中的一个RDD对于另一个RDD中存在的某个键没有对应的记录，那么对应的迭代器则为空，cogroup（）提供了为多个RDD进行数据分组的方法。

普通的join连接符表示内连接，只有在两个pair RDD中都存在的键才叫输出。当一个输入对应的某个键有多个值时，生成的pair RDD会包括来自两个输入RDD的每一组相对应的记录。

### Pair RDD的行动操作

以键值对集合{（1,2），（3,4），（3,6）为例}

|函数|描述|示例|结果|
|---|---|---|---|
|countByKey（）|对每个键对应的元素分别计数|rdd.countByKey( )|{(1,1),(3,2)}|
|collectAsMap（）|将结果以映射表的形式返回，以便查询|rdd.collectAsMap( )|Map{(1,2),(3,2),(3,6)}|
|lookup(key)|返回给定键对应的所有值|rdd.lookup(3)|[4,6]|

### Scala自定义分区方式

val sc = new SparkContext(...)

val  userData =

sc.sequenceFile[UserID,UserInfo]("hdfs://...").partitionBy(new

HashPartitioner(100)).persist( )

对userData表使用partitionBy（）转化操作，将这张表转为哈希分区，可以通过向partitionBy传递一个Spark.HashPartitioner对象来实现该操作。

由于在构建userData时调用了partitionBy（），Spark就知道了该RDD是根据键的哈希值来分区的，这样在join（）时，Spark就会利用到这一点。

注意，partitionBy（）是一个转化操作，因此它的返回值总是一个新的RDD，但它不会改变原来的RDD。RDD一旦创建就无法修改。因此应该对partitionBy（）的结果进行持久化，并保存为userData（）。此外，传给partitionBy（）的100表示分区数目，它会控制之后对这个RDD进行进一步操作（比如连接操作）时有多少任务会并行执行。总的来说，这个值至少应该和集群中的总核心数一样。

获取RDD的分区方式

在Scala和Java中，你可以使用RDD的partitioner属性（Java中使用partitioner（）方法）来获取RDD的分区方式，它会返回一个scala.Option对象，这是Scala中用来存放可能存在的对象的容器类。你可以对这个Option对象调用isDefined（）来检查其中是否有值，调用get（）来获取其中的值，如果存在值的话，这个值会是一个spark.Partitioner对象，这本质上是一个告诉我们RDD中各个键分别属于哪个分区的函数。


影响分区方式的操作

所有会为生成的结果RDD设好分区方式的操作：cogroup（）、groupWith（）、join（）、leftOuterJoin（）、rightOuterJoin（）、groupByKey（）、reduceByKey（）、combineByKey（）、partitionBy（）、sort（）、mapValues（）（如果父RDD有分区方式的话）、flatMapValues（）（如果父RDD有分区方式的话）、以及filter（）（如果父RDD有分区方式的话），其他所有的操作生成的结果都不会存在特定的分区方式。

最后，对于二元操作，输出数据的分区方式取决于父RDD的分区方式。默认情况下，结果会采用哈希分区，分区的数量和操作的并行度一样。不过，如果其中的一个父RDD已经设置过分区方式，那么结果就会采用那种分区方式，如果两个父RDD都设置过分区方式，则RDD采用第一个父RDD的分区方式。

自定义分区方式

继承org.apache.spark.Partitioner类并实现下面三个方法：
numPartition:Int：返回创建出来的分区数
getPartition(Key:Any):Int:返回给定键的分区符号（0到numPartitions-1）

equals（）：java判断相等性的标准方法。判断你的分区器对象是否与其他分区器实例相同，这样Spark才可以判断两个RDD的分区方式是否相同
